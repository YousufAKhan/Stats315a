{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stats315a_HWK1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJE6mhv_wiwz"
      },
      "source": [
        "Members: Adam Papini, Gary Burnett, Yousuf A. Khan\n",
        "\n",
        "The code below is for problem #1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRX1Uq9EwqS1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afe4dcc4-be4b-4b2e-c496-b930598d1de0"
      },
      "source": [
        "library(class)\n",
        "library(MASS)\n",
        "\n",
        "# ===== PROBLEM 1 =====\n",
        "# ------ PART A ------ \n",
        "\n",
        "# initialize some parameters from the problem \n",
        "pi = 0.5 \n",
        "K = 8 \n",
        "omega = rep(1/8, K) #rep function creates K length vector of 1/8\n",
        "sigma= 0.5 \n",
        "sigma2 = sigma^2 \n",
        "\n",
        "# generate location vectors \n",
        "initial_means = rbind(c(0,1), c(1,0)) # the means we will use to generate mu, 2x2 matrix of means \n",
        "mu = matrix(0, nrow = 0, ncol = 2) # initialize empty matrix \n",
        "\n",
        "# the first K rows of mu will be for class 0 \n",
        "# the second K rows of mu will be for class 1 \n",
        "for (i in 1:2){ \n",
        "  mu_x = rnorm(K, initial_means[i,1], sigma) # first component of mu for class i \n",
        "  mu_y = rnorm(K, initial_means[i,2], sigma) # second component of mu for class i \n",
        "  mu = rbind(mu, cbind(mu_x, mu_y)) \n",
        "}\n",
        "\n",
        "# split the class means from each other \n",
        "mu_0 = mu[1:K,] # mu's (location vectors) for class 0 \n",
        "mu_1 = mu[(K+1):(2*K),] # mu's (location vectors) for class 1 \n",
        "\n",
        "# print the result \n",
        "print('location vectors for class 0 are')\n",
        "print(mu_0)\n",
        "\n",
        "print('location vectors for class 1 are')\n",
        "print(mu_1)\n",
        "\n",
        "print(sigma)\n",
        "print(sigma^2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1] \"location vectors for class 0 are\"\n",
            "            mu_x      mu_y\n",
            "[1,]  0.57730319 0.8301554\n",
            "[2,] -0.44241563 1.1932145\n",
            "[3,] -0.16932508 0.8680880\n",
            "[4,]  0.59194676 0.9699000\n",
            "[5,]  0.13483254 1.6685943\n",
            "[6,]  0.08101815 1.4380567\n",
            "[7,]  0.26955509 1.0816383\n",
            "[8,]  0.68652236 1.2724779\n",
            "[1] \"location vectors for class 1 are\"\n",
            "           mu_x        mu_y\n",
            "[1,] 2.26121533  0.52154136\n",
            "[2,] 0.90685524  0.61707022\n",
            "[3,] 0.85656188  0.87328951\n",
            "[4,] 0.68411953 -0.07402917\n",
            "[5,] 1.20538204 -0.61736068\n",
            "[6,] 0.08202829 -0.48737437\n",
            "[7,] 0.89545057  0.09437323\n",
            "[8,] 1.96493691 -0.74700770\n",
            "[1] 0.5\n",
            "[1] 0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeikHMup0fMA"
      },
      "source": [
        "# ------ PART B ------ \n",
        "\n",
        "generate_data_points = function(y_input, centroid, N, pi, omega, sigma) {\n",
        "  \n",
        "  X = matrix(0, nrow = 0, ncol = 2) # initialize empty matrix to store output \n",
        "\n",
        "  for (i in (1:length(y_input))){\n",
        "\n",
        "    k = floor(runif(1, min=1, max=9)) # get a random number between 0 and 8\n",
        "\n",
        "    if (y_input[i] == 0){\n",
        "\n",
        "      # find a random value 1:8 to choose for the gaussian\n",
        "      # given the chosen K (5), choose one gaussian with one location vector (mu_5 --> sample one point from mu_5)\n",
        "\n",
        "      single_point_0 = rnorm(1, centroid[k,1], sigma) \n",
        "      single_point_1 = rnorm(1, centroid[k,2], sigma) \n",
        "\n",
        "      } else {\n",
        "\n",
        "      single_point_0 = rnorm(1, centroid[k+8,1], sigma) \n",
        "      single_point_1 = rnorm(1, centroid[k+8,2], sigma) \n",
        "      }\n",
        "    \n",
        "    single_point = c(single_point_0, single_point_1) \n",
        "    X = rbind(X, pi*single_point) # don't forget to multiple by pi \n",
        "  }\n",
        "  \n",
        "return(X) # each row is a new data point \n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDbI6lDly2v9"
      },
      "source": [
        "# ------ PART C ------\n",
        "\n",
        "num_train = 300 \n",
        "num_test = 20000\n",
        "\n",
        "ytrain = sample(c(0,1), replace=TRUE, size=num_train)\n",
        "ytest = sample(c(0,1), replace=TRUE, size=num_test) \n",
        "\n",
        "xtrain = generate_data_points(ytrain, mu, num_train, 0.5, omega, sigma2) \n",
        "xtest = generate_data_points(ytest, mu, num_test, 0.5, omega, sigma2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu9RNv48xMg8"
      },
      "source": [
        "# ------ PART D ------\n",
        "\n",
        "\n",
        "Bayes_classifier = function(means, sigma, X){\n",
        "  y_pred = matrix(0, nrow = nrow(X), ncol = 1) # initialize empty matrix to store output\n",
        "  K = nrow(means)/2\n",
        "\n",
        "  class0_prior = 0.5 #the probability of our data being class 0 without seeing any features\n",
        "  class1_prior = 0.5 #the probability of our data being class 1 without seeing any features\n",
        "\n",
        "  # separate into class0 and class1\n",
        "  means_class0 <- means[1:K,] \n",
        "  means_class1 <- means[(K+1):(2*K),]\n",
        "\n",
        "  # go through formula (1)\n",
        "  for (row in 1:nrow(X)){\n",
        "    x = X[row,1]\n",
        "    y = X[row,2]\n",
        "    class0_prob = 0\n",
        "    class1_prob = 0\n",
        "    for (mrow in 1:K){\n",
        " \n",
        "      x0_prob = dnorm(x, mean = means_class0[mrow,1], sd = sigma)\n",
        "      y0_prob = dnorm(y, mean = means_class0[mrow,2], sd = sigma)\n",
        "\n",
        "      x1_prob = dnorm(x, mean = means_class1[mrow,1], sd = sigma)\n",
        "      y1_prob = dnorm(y, mean = means_class1[mrow,2], sd = sigma)\n",
        "\n",
        "      class0_prob = class0_prob + (x0_prob*y0_prob)\n",
        "      class1_prob = class1_prob + (x1_prob*y1_prob)\n",
        "    }\n",
        "    if (class0_prob > class1_prob){\n",
        "      y_pred[row,1] = 0\n",
        "\n",
        "    }\n",
        "    else {\n",
        "      y_pred[row,1] = 1\n",
        "    }\n",
        "\n",
        "  }\n",
        "  # return the single column matrix of y prediction\n",
        "  return(y_pred)\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASW5x7NvxMjh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a43adf13-cb24-4eca-aafe-27093707ffae"
      },
      "source": [
        "# ------ PART E ------\n",
        "\n",
        "evaluate_classifiers = function(xtrain, xtest, ytrain, ytest, k){\n",
        "  \n",
        "  # calculate Bayes error\n",
        "  bayes_ytest_preds = Bayes_classifier(mu, sigma, xtest)\n",
        "  bayes_test_error = mean((ytest-bayes_ytest_preds)^2)\n",
        "\n",
        "\n",
        "  cat('Bayes test error is', bayes_test_error, '\\n')\n",
        "  \n",
        "  # calculate least squares error\n",
        "  dataset = as.data.frame(cbind(xtrain, ytrain)) # format the input data for lm function \n",
        "  ls_model = lm(ytrain~V1+V2, data=dataset) # train the ls model \n",
        "  ls_preds = predict(ls_model, as.data.frame(xtest)) # predict on the test data \n",
        "  ls_test_error = mean((ytest - ls_preds)^2) # calculate the test error \n",
        "  cat('Least Squares test error is', ls_test_error, '\\n')\n",
        "  \n",
        "  # calculate KNN test error \n",
        "  knn_error = matrix(0, nrow=1, ncol=k)\n",
        "  for (i in 1:length(k)){\n",
        "    knn_pred = knn(xtrain, xtest, factor(ytrain), k=k[i]) # predict using knn \n",
        "    knn_pred = as.numeric(levels(knn_pred))[knn_pred] # convert from factor to numeric to calculate mse \n",
        "    knn_error[i] = mean((ytest - knn_pred)^2) # calculate knn test error \n",
        "    cat('KNN test error is', knn_error[i], 'on', k[i], 'clusters \\n')\n",
        "  }\n",
        "  return(knn_error)\n",
        "}\n",
        "\n",
        "k = c(1, 3, 5, 7, 9, 11, 13, 15)\n",
        "knn_error = evaluate_classifiers(xtrain, xtest, ytrain, ytest, k)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bayes test error is 0.1506 \n",
            "Least Squares test error is 0.1041706 \n",
            "KNN test error is 0.17485 on 1 clusters \n",
            "KNN test error is 0.15085 on 3 clusters \n",
            "KNN test error is 0.1457 on 5 clusters \n",
            "KNN test error is 0.14015 on 7 clusters \n",
            "KNN test error is 0.1415 on 9 clusters \n",
            "KNN test error is 0.1404 on 11 clusters \n",
            "KNN test error is 0.136 on 13 clusters \n",
            "KNN test error is 0.137 on 15 clusters \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT-1AxzH33fU"
      },
      "source": [
        "# ------ PART F ------\n",
        "\n",
        "evaluate_classifiers_modified = function(xtrain, xtest, ytrain, ytest, k, noise, sigma.noise){\n",
        "  \n",
        "  # add gaussian noise to xtrain and xtest\n",
        "  cat('\\nAdding', noise, 'columns of noise to the data with sigma.noise =', sigma.noise, '\\n')\n",
        "  \n",
        "  num_train = dim(xtrain)[1] # number of rows in the training data \n",
        "  num_test = dim(xtest)[1] # number of rows in the test data \n",
        "  xtrain_noise = matrix(0, nrow = num_train, ncol = 0) # initialize empty matrix for training noise \n",
        "  xtest_noise = matrix(0, nrow = num_test, ncol = 0) # initialize empty matrix for testing noise \n",
        "  for (i in 1:noise){ # generate noise columns \n",
        "    xtrain_noise = cbind(xtrain_noise, rnorm(num_train, 0, sigma.noise)) \n",
        "    xtest_noise = cbind(xtest_noise, rnorm(num_test, 0, sigma.noise))\n",
        "  }\n",
        "  xtrain = cbind(xtrain, xtrain_noise) # append noisy columns to train data \n",
        "  xtest = cbind(xtest, xtest_noise) # append noisy columns to test data \n",
        "  \n",
        "  # calculate Bayes error \n",
        "  bayes_ytest_preds = Bayes_classifier(mu, sigma, xtest)\n",
        "  bayes_test_error = mean((ytest-bayes_ytest_preds)^2)\n",
        "  cat('Bayes test error is', bayes_test_error, '\\n')\n",
        "  \n",
        "  # calculate least squares error\n",
        "  dataset = as.data.frame(cbind(xtrain, ytrain)) # format the input data for lm function  \n",
        "  ls_model = lm(ytrain ~ ., data=dataset) # train the ls model \n",
        "  ls_preds = predict(ls_model, as.data.frame(xtest)) # predict on the test data \n",
        "  ls_test_error = mean((ytest - ls_preds)^2) # calculate the test error \n",
        "  cat('Least Squares test error is', ls_test_error, '\\n')\n",
        "  \n",
        "  # calculate KNN test error \n",
        "  knn_noise_error = matrix(0, nrow=1, ncol=length(k)) \n",
        "  for (i in 1:length(k)){\n",
        "    knn_pred = knn(xtrain, xtest, factor(ytrain), k=k[i]) # predict using knn \n",
        "    knn_pred = as.numeric(levels(knn_pred))[knn_pred] # convert from factor to numeric to calculate mse \n",
        "    knn_noise_error[i] = mean((ytest - knn_pred)^2) # calculate knn test error \n",
        "    cat('KNN test error is', knn_noise_error[i], 'on', k[i], 'clusters \\n')\n",
        "  }\n",
        "  return(knn_noise_error)\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rreSaDo4uWMv",
        "outputId": "81d78725-9dfa-4a87-9b54-40cc6acd6290"
      },
      "source": [
        "# ------ PART G ------\n",
        "\n",
        "k = c(1, 3, 5, 7, 9, 11, 13, 15)\n",
        "knn_noise_error = matrix(0, nrow=10, ncol=length(k))\n",
        "for (n in 1:10){\n",
        "  knn_noise_error[n,] = evaluate_classifiers_modified(xtrain, xtest, ytrain, ytest, k, noise=n, sigma.noise=1)\n",
        "}\n",
        "\n",
        "knn_error = rbind(knn_error, knn_noise_error)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Adding 1 columns of noise to the data with sigma.noise = 1 \n",
            "Bayes test error is 0.15455 \n",
            "Least Squares test error is 0.1051692 \n",
            "KNN test error is 0.19125 on 1 clusters \n",
            "KNN test error is 0.1747 on 3 clusters \n",
            "KNN test error is 0.1714 on 5 clusters \n",
            "KNN test error is 0.16865 on 7 clusters \n",
            "KNN test error is 0.16655 on 9 clusters \n",
            "KNN test error is 0.16375 on 11 clusters \n",
            "KNN test error is 0.16185 on 13 clusters \n",
            "KNN test error is 0.15705 on 15 clusters \n",
            "\n",
            "Adding 2 columns of noise to the data with sigma.noise = 1 \n",
            "Bayes test error is 0.15455 \n",
            "Least Squares test error is 0.1054731 \n",
            "KNN test error is 0.2263 on 1 clusters \n",
            "KNN test error is 0.20595 on 3 clusters \n",
            "KNN test error is 0.199 on 5 clusters \n",
            "KNN test error is 0.19395 on 7 clusters \n",
            "KNN test error is 0.19155 on 9 clusters \n",
            "KNN test error is 0.1875 on 11 clusters \n",
            "KNN test error is 0.1841 on 13 clusters \n",
            "KNN test error is 0.18285 on 15 clusters \n",
            "\n",
            "Adding 3 columns of noise to the data with sigma.noise = 1 \n",
            "Bayes test error is 0.15455 \n",
            "Least Squares test error is 0.1052178 \n",
            "KNN test error is 0.2724 on 1 clusters \n",
            "KNN test error is 0.2476 on 3 clusters \n",
            "KNN test error is 0.23665 on 5 clusters \n",
            "KNN test error is 0.22675 on 7 clusters \n",
            "KNN test error is 0.22205 on 9 clusters \n",
            "KNN test error is 0.21995 on 11 clusters \n",
            "KNN test error is 0.22385 on 13 clusters \n",
            "KNN test error is 0.2249 on 15 clusters \n",
            "\n",
            "Adding 4 columns of noise to the data with sigma.noise = 1 \n",
            "Bayes test error is 0.15455 \n",
            "Least Squares test error is 0.1049807 \n",
            "KNN test error is 0.3106 on 1 clusters \n",
            "KNN test error is 0.2773 on 3 clusters \n",
            "KNN test error is 0.2668 on 5 clusters \n",
            "KNN test error is 0.25845 on 7 clusters \n",
            "KNN test error is 0.25745 on 9 clusters \n",
            "KNN test error is 0.25755 on 11 clusters \n",
            "KNN test error is 0.25605 on 13 clusters \n",
            "KNN test error is 0.25925 on 15 clusters \n",
            "\n",
            "Adding 5 columns of noise to the data with sigma.noise = 1 \n",
            "Bayes test error is 0.15455 \n",
            "Least Squares test error is 0.1047753 \n",
            "KNN test error is 0.3315 on 1 clusters \n",
            "KNN test error is 0.31275 on 3 clusters \n",
            "KNN test error is 0.3006 on 5 clusters \n",
            "KNN test error is 0.29575 on 7 clusters \n",
            "KNN test error is 0.2927 on 9 clusters \n",
            "KNN test error is 0.2875 on 11 clusters \n",
            "KNN test error is 0.28685 on 13 clusters \n",
            "KNN test error is 0.28475 on 15 clusters \n",
            "\n",
            "Adding 6 columns of noise to the data with sigma.noise = 1 \n",
            "Bayes test error is 0.15455 \n",
            "Least Squares test error is 0.106566 \n",
            "KNN test error is 0.3675 on 1 clusters \n",
            "KNN test error is 0.34875 on 3 clusters \n",
            "KNN test error is 0.33755 on 5 clusters \n",
            "KNN test error is 0.33005 on 7 clusters \n",
            "KNN test error is 0.32765 on 9 clusters \n",
            "KNN test error is 0.3237 on 11 clusters \n",
            "KNN test error is 0.31775 on 13 clusters \n",
            "KNN test error is 0.3194 on 15 clusters \n",
            "\n",
            "Adding 7 columns of noise to the data with sigma.noise = 1 \n",
            "Bayes test error is 0.15455 \n",
            "Least Squares test error is 0.1056836 \n",
            "KNN test error is 0.3763 on 1 clusters \n",
            "KNN test error is 0.3564 on 3 clusters \n",
            "KNN test error is 0.3452 on 5 clusters \n",
            "KNN test error is 0.34205 on 7 clusters \n",
            "KNN test error is 0.3382 on 9 clusters \n",
            "KNN test error is 0.3304 on 11 clusters \n",
            "KNN test error is 0.33075 on 13 clusters \n",
            "KNN test error is 0.33245 on 15 clusters \n",
            "\n",
            "Adding 8 columns of noise to the data with sigma.noise = 1 \n",
            "Bayes test error is 0.15455 \n",
            "Least Squares test error is 0.1113318 \n",
            "KNN test error is 0.39485 on 1 clusters \n",
            "KNN test error is 0.38125 on 3 clusters \n",
            "KNN test error is 0.36915 on 5 clusters \n",
            "KNN test error is 0.3622 on 7 clusters \n",
            "KNN test error is 0.35685 on 9 clusters \n",
            "KNN test error is 0.35625 on 11 clusters \n",
            "KNN test error is 0.35465 on 13 clusters \n",
            "KNN test error is 0.35085 on 15 clusters \n",
            "\n",
            "Adding 9 columns of noise to the data with sigma.noise = 1 \n",
            "Bayes test error is 0.15455 \n",
            "Least Squares test error is 0.1066686 \n",
            "KNN test error is 0.4097 on 1 clusters \n",
            "KNN test error is 0.39515 on 3 clusters \n",
            "KNN test error is 0.38695 on 5 clusters \n",
            "KNN test error is 0.3846 on 7 clusters \n",
            "KNN test error is 0.3808 on 9 clusters \n",
            "KNN test error is 0.38345 on 11 clusters \n",
            "KNN test error is 0.3875 on 13 clusters \n",
            "KNN test error is 0.3857 on 15 clusters \n",
            "\n",
            "Adding 10 columns of noise to the data with sigma.noise = 1 \n",
            "Bayes test error is 0.15455 \n",
            "Least Squares test error is 0.1069531 \n",
            "KNN test error is 0.4277 on 1 clusters \n",
            "KNN test error is 0.4098 on 3 clusters \n",
            "KNN test error is 0.40155 on 5 clusters \n",
            "KNN test error is 0.39575 on 7 clusters \n",
            "KNN test error is 0.3913 on 9 clusters \n",
            "KNN test error is 0.39045 on 11 clusters \n",
            "KNN test error is 0.38945 on 13 clusters \n",
            "KNN test error is 0.38765 on 15 clusters \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXriMHfGuKyZ"
      },
      "source": [
        "Part G (Write-Up) \n",
        "\n",
        "As we increase the number of noise columns, we don't expect any change to the bayes test error. Bayes Classifier only relies on the prior probabilities of each class, and therefore is not influenced by noise in the training data. For least squares we expect a similar result, since the noise we are adding is relatively small and doesn't prevent the classes from being linearly separable. This is in directy contrast to KNN, which will be greatly affected by the addition of noise columns. We can see in our results that with no noise, the KNN is on par with the Bayes and Linear Classifiers, however as we increase the noise we get up to around 40% test error with 10 columns of noise. This is a significant change in performance. Another trend we notice with KNN, is that as you increase the number of neighbors, the test error decreases. This makes sense, as the classifier has more information to base its prediction off of. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nxH-UjYXBPQ"
      },
      "source": [
        "The code below is for HWK problem #3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1QwF-iq35SZ"
      },
      "source": [
        "# ===== PROBLEM 3 =====\n",
        "# ------ PART A ------ \n",
        "estimate_x0 = function(desired_value, N){\n",
        "  x <- c(runif(N, min=0, max=1)) # get 50 random x's\n",
        "  epsilon <- c(rnorm(N, 0, 0.2^2)) # get 50 random epsilon's\n",
        "  f_x = (1+x^2) # our signal generator\n",
        "  y <- f_x +epsilon # our signal plus noise\n",
        "  d1<-data.frame(x,y) # convert into a dataframe\n",
        "  model <- lm(y ~ poly(x,2), data = d1) # polynomial regression\n",
        "  \n",
        "  xval <- approx(x = model$fitted.values, y = x, xout = desired_value)$y\n",
        "  \n",
        "  return(c(xval,x,y))\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CsudkaY35Ux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fa70db5-bf1b-46b5-c804-b516365bf28f"
      },
      "source": [
        "# ------ PART B ------ \n",
        "N = 50\n",
        "val = 1.3\n",
        "x0_single = estimate_x0(val,N)\n",
        "cat('The predicted value is for a y of 1.3 is an x = ', x0_single[1])\n",
        "x_original = x0_single[2:51]\n",
        "y_original = x0_single[52:101]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The predicted value is for a y of 1.3 is an x =  0.5475735"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBjbWuJ235W9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c510a05d-9041-4401-8dfc-d081c2a51f10"
      },
      "source": [
        "# ------ PART C ------\n",
        "x0_1000 <- c()\n",
        "for (i in 1:1000){\n",
        "  x0_1000 <- c(x0_1000,(estimate_x0(val,N))[1]) #add each value\n",
        "}\n",
        "\n",
        "cat('After 1000 simulations, our predicted value is', mean(x0_1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After 1000 simulations, our predicted value is 0.5473172"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf3RwflE35ZP"
      },
      "source": [
        "# ------ PART D ------\n",
        "B = 1000  #Number times to bootstrap from our sample of 50\n",
        "x0_bootstrap <- c()\n",
        "for (i in 1:B){\n",
        "  bootSample <- sample(1:50, 50, replace=TRUE)\n",
        "  x <- c()\n",
        "  y <- c()\n",
        "  for (i in bootSample){\n",
        "    x <- c(x,x_original[i])\n",
        "    y <- c(y,y_original[i])\n",
        "  }\n",
        "  d1_boot <-data.frame(x,y) #convert into a dataframe\n",
        "  bootModel <- lm(y ~ poly(x,2), data = d1_boot) #polynomial regression\n",
        "  xval <- approx(x = bootModel$fitted.values, y = x, xout = 1.3)$y\n",
        "  x0_bootstrap <- c(x0_bootstrap, xval)\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aklum1HH3wrJ",
        "outputId": "ace81fb1-f16b-459e-f0a3-bc1ee548b4bb"
      },
      "source": [
        "cat('The bootstrap prediction is ', mean(x0_bootstrap))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The bootstrap prediction is  0.5469935"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSz2gw2Y35bT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9b604d2-9e6b-4854-ebed-633c89eef5b7"
      },
      "source": [
        "# ------ PART E ------\n",
        "cat('The sampling distribution SD: ',sd(x0_1000), '\\n')\n",
        "cat('The bootstrap distribution SD: ', sd(x0_bootstrap))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sampling distribution SD:  0.007470314 \n",
            "The bootstrap distribution SD:  0.007663411"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lSlYKii3ddY"
      },
      "source": [
        "As you can see, the standard deviations for the sampling distribution and the bootstrap distribution are very close, as expected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MHYPIovgGr9"
      },
      "source": [
        "\n",
        "# Part F\n",
        "\n",
        "1) The first way we will do this is simply take the bottom 10% and the top 10% values as our measures of our confidence interval. The median will serve as our 'middle'.\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV9mSuilhI85",
        "outputId": "5d92ca7c-ba9e-42bf-ab55-c03716c375d5"
      },
      "source": [
        "sorted_bootstrap <- sort(x0_bootstrap, decreasing = FALSE)\n",
        "cat('The lower CI is: ', sorted_bootstrap[100], '\\n')\n",
        "cat('The point estimate is: ', sorted_bootstrap[500], '\\n')\n",
        "cat('The upper CI is: ', sorted_bootstrap[900])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The lower CI is:  0.5373687 \n",
            "The point estimate is:  0.5465321 \n",
            "The upper CI is:  0.5569746"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEaPSZrejqzc"
      },
      "source": [
        "2) An alternative method would be to assume that we have enough bootstrap samples to assume that the results are normally distributied therefore we could use the central limit theorm and calculate the 90% CIs from a normal value table. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXN5LCPLmhkE",
        "outputId": "25de4a34-5f79-4c05-98ff-c5c7e366abbe"
      },
      "source": [
        "z_score <- 1.645*(sd(x0_bootstrap)/sqrt(1000))\n",
        "cat('The lower CI is: ', mean(x0_bootstrap) - z_score, '\\n')\n",
        "cat('The point estimate is: ', mean(x0_bootstrap), '\\n')\n",
        "cat('The upper CI is: ', mean(x0_bootstrap) + z_score)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The lower CI is:  0.5465949 \n",
            "The point estimate is:  0.5469935 \n",
            "The upper CI is:  0.5473922"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OXEuJrj9E-A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}